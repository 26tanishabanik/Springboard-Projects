{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-label image classification:\n",
    "- Assign one or more class labels must be predicted for each image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "#%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths to data\n",
    "PROJ_ROOT = os.path.join(os.pardir)\n",
    "\n",
    "jpg_image_path = str(os.path.join(PROJ_ROOT,\n",
    "                            'data',\n",
    "                            'raw',\n",
    "                             'train-jpg')\n",
    "                            + '\\\\{}' + '.jpg')\n",
    "\n",
    "jpg_image_path_folder = str(os.path.join(PROJ_ROOT,\n",
    "                            'data',\n",
    "                            'raw',\n",
    "                             'train-jpg'))\n",
    "\n",
    "tif_image_path = str(os.path.join(PROJ_ROOT,\n",
    "                            'data',\n",
    "                            'raw',\n",
    "                             'train-tif-v2')\n",
    "                            + '\\\\{}' + '.tif')\n",
    "\n",
    "tif_image_path_folder = str(os.path.join(PROJ_ROOT,\n",
    "                            'data',\n",
    "                            'raw',\n",
    "                             'train-tif-v2'))\n",
    "\n",
    "image_tags_path = os.path.join(PROJ_ROOT,\n",
    "                            'data',\n",
    "                            'raw',\n",
    "                             'train_v2.csv')\n",
    "\n",
    "save_data_path = str(os.path.join(PROJ_ROOT,\n",
    "                            'data',\n",
    "                            'processed')\n",
    "                             + '\\\\{}')\n",
    "\n",
    "save_model_path = str(os.path.join(PROJ_ROOT,\n",
    "                            'models')\n",
    "                             + '\\\\{}.h5')\n",
    "\n",
    "save_history_path = str(os.path.join(PROJ_ROOT,\n",
    "                            'models', 'histories')\n",
    "                             + '\\\\{}.pkl')\n",
    "\n",
    "save_predictions_path = str(os.path.join(PROJ_ROOT,\n",
    "                            'models', 'predictions')\n",
    "                             + '\\\\{}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping of tags to integers given the loaded mapping file\n",
    "def create_tag_mapping(tag_mapping_csv):\n",
    "    '''\n",
    "    Input the raw csv listing image files and their associated tags.\n",
    "    Output a dictionary that maps tags to integers, and a dictionary\n",
    "    that maps integers to tags (inverse of first dictionary).\n",
    "    '''\n",
    "    \n",
    "    # create a set of all known tags\n",
    "    all_tags = set()\n",
    "    \n",
    "    for i in range(len(tag_mapping_csv)):\n",
    "        \n",
    "        # convert spaced separated tags into an array of tags\n",
    "        tags = tag_mapping_csv['tags'][i].split(' ')\n",
    "        \n",
    "        # add tags to the set of known tags\n",
    "        all_tags.update(tags)\n",
    "        \n",
    "    # convert set of tags to a list to list\n",
    "    all_tags = list(all_tags)\n",
    "    \n",
    "    # order set alphabetically\n",
    "    all_tags.sort()\n",
    "    \n",
    "    # dict that maps tags to integers, and the reverse\n",
    "    tags_map = {all_tags[i]:i for i in range(len(all_tags))}\n",
    "    inv_tags_map = {i:all_tags[i] for i in range(len(all_tags))}\n",
    "    \n",
    "    return tags_map, inv_tags_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping of filename to a list of tags\n",
    "def create_file_mapping(tag_mapping_csv):\n",
    "    '''\n",
    "    Input the raw csv listing image files and their associated tags.\n",
    "    Output dictionary with image names as keys and split tags as values.\n",
    "    '''\n",
    "    mapping = dict()\n",
    "    for i in range(len(tag_mapping_csv)):\n",
    "        \n",
    "        name, tags = tag_mapping_csv['image_name'][i], tag_mapping_csv['tags'][i]\n",
    "        mapping[name] = tags.split(' ')\n",
    "        \n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a one hot encoding for one list of tags\n",
    "def one_hot_encode(tags, mapping):\n",
    "    '''\n",
    "    Input list of tags associated with an image and the tag mappings.\n",
    "    Output an array of 0s and 1s, having 1s where the index of the 1\n",
    "    matches the index of the mapping dicionary of that tag.\n",
    "    '''\n",
    "    # create empty vector\n",
    "    encoding = np.zeros(len(mapping), dtype='uint8')\n",
    "    \n",
    "    # mark 1 for each tag in the vector\n",
    "    for tag in tags:\n",
    "        encoding[mapping[tag]] = 1\n",
    "        \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all images into memory\n",
    "def make_dataset(path, file_mapping, tag_mapping, image_target_size=(16,16)):\n",
    "    '''\n",
    "    Input \n",
    "    ''' \n",
    "    photos, targets = list(), list()\n",
    "    \n",
    "    # enumerate files in the directory\n",
    "    for filename in tqdm(os.listdir(path)):\n",
    "        \n",
    "        # load image\n",
    "        photo = load_img(path + '\\\\' + filename, target_size=image_target_size)\n",
    "        \n",
    "        # convert to numpy array\n",
    "        photo = img_to_array(photo, dtype='uint8')\n",
    "        \n",
    "        # get tags\n",
    "        tags = file_mapping[filename[:-4]]\n",
    "        \n",
    "        # one hot encode tags\n",
    "        target = one_hot_encode(tags, tag_mapping)\n",
    "        \n",
    "        # store\n",
    "        photos.append(photo)\n",
    "        targets.append(target)\n",
    "        \n",
    "    X = np.asarray(photos, dtype='uint8')\n",
    "    y = np.asarray(targets, dtype='uint8')\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the mapping file\n",
    "filename = 'train_v2.csv'\n",
    "tag_mapping_csv = pd.read_csv(image_tags_path)\n",
    "\n",
    "# create a mapping of tags to integers\n",
    "tag_mapping, _ = create_tag_mapping(tag_mapping_csv)\n",
    "\n",
    "# create a mapping of filenames to tag lists\n",
    "file_mapping = create_file_mapping(tag_mapping_csv)\n",
    "\n",
    "# load the jpeg images\n",
    "folder = jpg_image_path_folder\n",
    "X, y = make_dataset(folder, file_mapping, tag_mapping, image_target_size=(128,128))\n",
    "\n",
    "print('X shape:', X.shape, 'y shape:', y.shape)\n",
    "\n",
    "# save both arrays to one file in compressed format\n",
    "np.savez_compressed(save_data_path.format('processed_data_full.npz'), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_mapping, inv_tags_map = create_tag_mapping(tag_mapping_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train and test dataset\n",
    "def load_dataset(dataset_name, test_size=0.3, random_state=42):\n",
    "    '''\n",
    "    Input the name of a previously created dataset (e.g. planet_data_5000.npz)\n",
    "    Split the dataset into train and test sets and return.\n",
    "    '''\n",
    "    \n",
    "    # load dataset\n",
    "    data = np.load(save_data_path.format(dataset_name))\n",
    "    X, y = data['arr_0'], data['arr_1']\n",
    "    \n",
    "    # separate into train and test datasets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    print('X_train shape:', X_train.shape,\n",
    "          'X_test shape:', X_test.shape,\n",
    "          'y_train shape:', y_train.shape,\n",
    "          'y_test shape:', y_test.shape)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "from sklearn.metrics import fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make all one predictions\n",
    "train_yhat = np.asarray([np.ones(y_train.shape[1]) for _ in range(y_train.shape[0])])\n",
    "test_yhat = np.asarray([np.ones(y_test.shape[1]) for _ in range(y_test.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_score = fbeta_score(y_train, train_yhat, 2, average='samples')\n",
    "test_score = fbeta_score(y_test, test_yhat, 2, average='samples')\n",
    "print('All Ones: train=%.3f, test=%.3f' % (train_score, test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Must create an fbeta score function that can be used by keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend\n",
    "\n",
    "# calculate fbeta score for multi-class/label classification\n",
    "def fbeta(y_true, y_pred, beta=2):\n",
    "    # clip predictions\n",
    "    y_pred = backend.clip(y_pred, 0, 1)\n",
    "    # calculate elements\n",
    "    tp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "    fp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n",
    "    fn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n",
    "    # calculate precision\n",
    "    p = tp / (tp + fp + backend.epsilon())\n",
    "    # calculate recall\n",
    "    r = tp / (tp + fn + backend.epsilon())\n",
    "    # calculate fbeta, averaged across each class\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n",
    "    return fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = fbeta(backend.variable(y_train), backend.variable(train_yhat)).eval(session=K.get_session())\n",
    "test_score = fbeta(backend.variable(y_test), backend.variable(test_yhat)).eval(session=K.get_session())\n",
    "print('All Ones (keras): train=%.3f, test=%.3f' % (train_score, test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VGG-type structure:\n",
    "https://machinelearningmastery.com/use-pre-trained-vgg-model-classify-objects-photographs/\n",
    "    - Blocks of convolutional layers with 3x3 filters, followed by max pooling layer - pattern repeats with a doubling of number of filters with each block added.\n",
    "    - Specifically, each block has two convolutional layers with 3x3 filters, ReLU activation, and He weight initialization (with same padding)\n",
    "    - Output feature maps have the same width and height \n",
    "    - These blocks are followed by a max pooling layer with 3x3 kernel\n",
    "    - 3 of these blocks are used with 32, 64, and 128 filters\n",
    "    - Output of final pooling layer is flattened and fed to fully connected layer for interpretation then lastly fed to an output layer for prediction.\n",
    "    - The model must produce a 17 element vector (17 tags/classes) with a prediction between 0 and 1 for each output tag/class.\n",
    "    - Sigmoid activation function in the output layer (multi-label classification) and optimize binary cross entropy loss function\n",
    "    - Model is optimized using mini-batch stochastic gradient descent \n",
    "        - Conservative learning rate of 0.01\n",
    "        - Momentum of 0.9\n",
    "        - Model keeps track of 'fbeta' metric during training\n",
    "        \n",
    "        \n",
    "- How it works:\n",
    "     - Convolution layers:\n",
    "         - Filter of small size is moved across the image and convolution operations are performed\n",
    "         - Convolutional operations are element-wise matrix multiplication between the filter values and the pixels in the image, with the resultant values being summed\n",
    "     - Pooling (downsampling) layers:\n",
    "         - Used to downsample the image\n",
    "         - Images contain many pixel values, it is typically easier for the network to learn the features if the image size is progressively reduced\n",
    "         -  Reduces number of parameters and therefore computation\n",
    "         - Also helps avoid overfitting\n",
    "         - Max pooling selects maximum value, average pooling (rarely used) takes an average...\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import timeit\n",
    "import pickle\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define cnn model\n",
    "def define_model(dropout, adam_opt, in_shape=(16, 16, 3), out_shape=17):\n",
    "    \n",
    "    # Initiate model as sequential - will be a linear stack of layers\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add first block\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=in_shape))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # dropout if set\n",
    "    if dropout == True:\n",
    "        model.add(Dropout(0.2))\n",
    "    \n",
    "    # Add second block\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    # dropout if set\n",
    "    if dropout == True:\n",
    "        model.add(Dropout(0.2))\n",
    "    \n",
    "    # Add third block\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # dropout if set\n",
    "    if dropout == True:\n",
    "        model.add(Dropout(0.2))\n",
    "    \n",
    "    # Flatten (to 1D vector) and feed to fully connected layer (relu activation)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    \n",
    "    # dropout if set\n",
    "    if dropout == True:\n",
    "        model.add(Dropout(0.5))\n",
    "    \n",
    "    # feed to output layer (sigmoid)\n",
    "    model.add(Dense(out_shape, activation='sigmoid'))\n",
    "    \n",
    "    # Initiate optimization\n",
    "    if adam_opt == True:\n",
    "        opt = Adam()\n",
    "    else:\n",
    "        opt = SGD(lr=0.01, momentum=0.9)\n",
    "    \n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(10,10))\n",
    "    \n",
    "    # plot loss\n",
    "    ax[0].set_title('Cross Entropy Loss')\n",
    "    ax[0].plot(history.history['loss'], color='blue')\n",
    "    ax[0].plot(history.history['val_loss'], color='orange')\n",
    "    ax[0].legend(['train', 'test'], loc='upper left')\n",
    "    \n",
    "    # plot accuracy\n",
    "    ax[1].set_title('Fbeta')\n",
    "    ax[1].plot(history.history['fbeta'], color='blue')\n",
    "    ax[1].plot(history.history['val_fbeta'], color='orange')\n",
    "    ax[1].legend(['train', 'test'], loc='upper left')\n",
    "    \n",
    "    # save plot to file\n",
    "    # filename = sys.argv[0].split('/')[-1]\n",
    "    # pyplot.savefig(filename + '_plot.png')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating models\n",
    "def run_test_harness(dataset_name,\n",
    "                     model_name,\n",
    "                     in_shape,\n",
    "                     epochs,\n",
    "                     dropout=False,\n",
    "                     augment=False,\n",
    "                     adam_opt=False):\n",
    "    \n",
    "    # load dataset\n",
    "    X_train, y_train, X_test, y_test = load_dataset(dataset_name)\n",
    "    \n",
    "    # create data generator(s)\n",
    "    if augment == True:\n",
    "        train_datagen = ImageDataGenerator(rescale=1.0/255.0, horizontal_flip=True, vertical_flip=True, rotation_range=90)\n",
    "        test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "        \n",
    "         # prepare iterators\n",
    "        train_it = train_datagen.flow(X_train, y_train, batch_size=128)\n",
    "        test_it = test_datagen.flow(X_test, y_test, batch_size=128)\n",
    "        \n",
    "    else:\n",
    "        # no augmentation\n",
    "        datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "        \n",
    "        # prepare iterators\n",
    "        train_it = datagen.flow(X_train, y_train, batch_size=128)\n",
    "        test_it = datagen.flow(X_test, y_test, batch_size=128)\n",
    "    \n",
    "    model = define_model(dropout=dropout, adam_opt=adam_opt, in_shape=in_shape, out_shape=17)\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
    "        validation_data=test_it, validation_steps=len(test_it), epochs=epochs, verbose=2)\n",
    "    \n",
    "    # evaluate model\n",
    "    loss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=2)\n",
    "    print('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))\n",
    "    \n",
    "    # save model\n",
    "    model.save(save_model_path.format(model_name))\n",
    "    \n",
    "    # learning curves\n",
    "    summarize_diagnostics(history)\n",
    "    \n",
    "    # make predictions for test data\n",
    "    predictions = model.predict_generator(test_it, steps=len(test_it))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(model_name, history, predictions):\n",
    "\n",
    "  with open(save_history_path.format(model_name), 'wb') as file:\n",
    "    pickle.dump(history, file)\n",
    "\n",
    "  with open(save_predictions_path.format(model_name), 'wb') as file:\n",
    "    pickle.dump(predictions, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "start = timeit.default_timer()\n",
    "\n",
    "dataset_name = 'processed_data_full.npz'\n",
    "model_name = 'base_model'\n",
    "in_shape = (16, 16, 3)\n",
    "base_predictions = run_test_harness(dataset_name,\n",
    "                                    model_name,\n",
    "                                    in_shape,\n",
    "                                    epochs=50,\n",
    "                                    dropout=False,\n",
    "                                    augment=False,\n",
    "                                    adam_opt=False)\n",
    "\n",
    "save_data(model_name, history, predictions)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', (stop - start) / 60)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Regularization\n",
    "- Computationally cheap method to regularize a deep neural network\n",
    "- Probabilistically remove (i.e. drop out) inputs to a layer\n",
    "- Effectively simulates a large number of networks with very different structures which makes the modes in a network generally more robust to the inputs\n",
    "- Avoid overfitting\n",
    "https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/\n",
    "- Can apply a small amount of dropout after each block - more dropout is applied to the fully connected layers (near the output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "start = timeit.default_timer()\n",
    "\n",
    "dataset_name = 'processed_data_full.npz'\n",
    "model_name = 'dropout_model.h5'\n",
    "in_shape = (128, 128, 3)\n",
    "# 200 epochs given that we expect rate of learning to slow with dropout\n",
    "run_test_harness(dataset_name,\n",
    "                 model_name,\n",
    "                 in_shape,\n",
    "                 epochs=200,\n",
    "                 dropout=True,\n",
    "                 augment=False,\n",
    "                 adam_opt=False)\n",
    "\n",
    "save_data(model_name, history, predictions)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', (stop - start) / 60)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Data Augmentation\n",
    "- Artificially expand size of training dataset by creating modified versions of images in the original dataset\n",
    "- Can help models ability to generalize\n",
    "- Can also act as a regularization method\n",
    "- Can be specified as arguments to the ImageDataGenerator\n",
    "- Should not be used on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "start = timeit.default_timer()\n",
    "\n",
    "dataset_name = 'processed_data_full.npz'\n",
    "model_name = 'augment_model.h5'\n",
    "in_shape = (128, 128, 3)\n",
    "run_test_harness(dataset_name,\n",
    "                 model_name,\n",
    "                 in_shape,\n",
    "                 epochs=200,\n",
    "                 dropout=False,\n",
    "                 augment=True,\n",
    "                 adam_opt=False)\n",
    "\n",
    "save_data(model_name, history, predictions)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', (stop - start) / 60)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Regularization and Image Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "start = timeit.default_timer()\n",
    "\n",
    "dataset_name = 'processed_data_full.npz'\n",
    "model_name = 'dropout_augment_model.h5'\n",
    "in_shape = (128, 128, 3)\n",
    "run_test_harness(dataset_name,\n",
    "                 model_name,\n",
    "                 in_shape,\n",
    "                 epochs=200,\n",
    "                 dropout=True,\n",
    "                 augment=True,\n",
    "                 adam_opt=False)\n",
    "\n",
    "save_data(model_name, history, predictions)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', (stop - start) / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Optimizer\n",
    "- ............................"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "start = timeit.default_timer()\n",
    "\n",
    "dataset_name = 'planet_data_5000.npz'\n",
    "model_name = 'adam_model.h5'\n",
    "in_shape = (128, 128, 3)\n",
    "run_test_harness(dataset_name,\n",
    "                 model_name,\n",
    "                 in_shape,\n",
    "                 epochs=50,\n",
    "                 dropout=False,\n",
    "                 augment=False,\n",
    "                 adam_opt=True)\n",
    "\n",
    "save_data(model_name, history, predictions)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', (stop - start) / 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Optimizer and dropout regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "start = timeit.default_timer()\n",
    "\n",
    "dataset_name = 'planet_data_5000.npz'\n",
    "model_name = 'adam_dropout_model.h5'\n",
    "in_shape = (128, 128, 3)\n",
    "run_test_harness(dataset_name,\n",
    "                 model_name,\n",
    "                 in_shape,\n",
    "                 epochs=200,\n",
    "                 dropout=True,\n",
    "                 augment=False,\n",
    "                 adam_opt=True)\n",
    "\n",
    "save_data(model_name, history, predictions)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', (stop - start) / 60)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Optimizer and image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "start = timeit.default_timer()\n",
    "\n",
    "dataset_name = 'planet_data_5000.npz'\n",
    "model_name = 'adam_augment_model.h5'\n",
    "in_shape = (128, 128, 3)\n",
    "run_test_harness(dataset_name,\n",
    "                 model_name,\n",
    "                 in_shape,\n",
    "                 epochs=200,\n",
    "                 dropout=False,\n",
    "                 augment=True,\n",
    "                 adam_opt=True)\n",
    "\n",
    "save_data(model_name, history, predictions)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', (stop - start) / 60)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Optimizer, dropout regularization, and image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "start = timeit.default_timer()\n",
    "\n",
    "dataset_name = 'processed_data_full.npz'\n",
    "model_name = 'adam_dropout_augment_model'\n",
    "in_shape = (128, 128, 3)\n",
    "run_test_harness(dataset_name,\n",
    "                 model_name,\n",
    "                 in_shape,\n",
    "                 epochs=200,\n",
    "                 dropout=True,\n",
    "                 augment=True,\n",
    "                 adam_opt=True)\n",
    "\n",
    "save_data(model_name, history, predictions)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', (stop - start) / 60)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the model further..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_labels = []\n",
    "_, inv_tags_map = create_tag_mapping(tag_mapping_csv)\n",
    "    \n",
    "for prediction in predictions:\n",
    "        \n",
    "    labels = [inv_tags_map[i] for i, value in enumerate(prediction) if value > thresholds[i]]\n",
    "    prediction_labels.append(labels)\n",
    "    \n",
    "plot_prediction_by_tag(prediction_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(filename):\n",
    "    # load the image\n",
    "    img = load_img(filename, target_size=(128, 128))\n",
    "    # convert to array\n",
    "    img = img_to_array(img)\n",
    "    # reshape into a single sample with 3 channels\n",
    "    img = img.reshape(1, 128, 128, 3)\n",
    "    # center pixel data\n",
    "    img = img.astype('float32')\n",
    "    img = img - [123.68, 116.779, 103.939]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the class\n",
    "# result = model.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a prediction to tags\n",
    "thresholds = [0.2] * len(tag_mapping)\n",
    "\n",
    "def prediction_to_tags(inv_mapping, prediction):\n",
    "    # round probabilities to {0, 1}\n",
    "    values = prediction.round()\n",
    "    # collect all predicted tags\n",
    "    tags = [inv_mapping[i] for i in range(len(values)) if values[i] == 1.0]\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an image and predict the class\n",
    "def run_example(inv_mapping):\n",
    "    # load the image\n",
    "    img = load_image('sample_image.jpg')\n",
    "    # load model\n",
    "    model = load_model('final_model.h5')\n",
    "    # predict the class\n",
    "    result = model.predict(img)\n",
    "    print(result[0])\n",
    "    # map prediction to tags\n",
    "    tags = prediction_to_tags(inv_mapping, result[0])\n",
    "    print(tags)\n",
    " \n",
    "# load the mapping file\n",
    "filename = 'train_v2.csv'\n",
    "mapping_csv = read_csv(filename)\n",
    "# create a mapping of tags to integers\n",
    "_, inv_mapping = create_tag_mapping(mapping_csv)\n",
    "# entry point, run the example\n",
    "run_example(inv_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
